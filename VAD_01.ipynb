{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtCYk9Zgam3e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxcQPXFSOLgK"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "  DATASET_PATH =\"/content/drive/MyDrive/UCSD_Anomaly_Dataset.v1p2/UCSDped2/Train\"\n",
        "  SINGLE_TEST_PATH = \"/content/drive/MyDrive/UCSD_Anomaly_Dataset.v1p2/UCSDped2/Train/Train007\"\n",
        "  BATCH_SIZE = 4\n",
        "  EPOCHS = 40\n",
        "  MODEL_PATH = \"/content/drive/MyDrive/model.hdf5\"\n",
        "  TEST_SIZE = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CjAkmkjJlYm",
        "outputId": "a2e3efce-1530-46da-a362-34b4e97cdd56"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "gpu_available = tf.test.is_gpu_available()\n",
        "\n",
        "print(\"GPU available:\", gpu_available)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wleP_qtYZvHV",
        "outputId": "f6ab594b-ed92-4dc2-fb03-a3925189755d"
      },
      "outputs": [],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {} '.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyI7e5axteDp",
        "outputId": "94aece06-1e47-4d71-b4ee-3570504df5ba"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSoua4TcOe1r"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def get_clips_by_stride(stride, frames_list, sequence_size):\n",
        "    \"\"\"For data augmenting purposes.\"\"\"\n",
        "    clips = []\n",
        "    sz = len(frames_list)\n",
        "    clip = np.zeros(shape=(sequence_size, 256, 256, 1))\n",
        "    cnt = 0\n",
        "    for start in range(0, stride):\n",
        "        for i in range(start, sz, stride):\n",
        "            clip[cnt, :, :, 0] = frames_list[i]\n",
        "            cnt = cnt + 1\n",
        "            if cnt == sequence_size:\n",
        "                clips.append(np.copy(clip))\n",
        "                cnt = 0\n",
        "    return clips\n",
        "\n",
        "def get_dataset():\n",
        "    \"\"\"Prepare the dataset.\"\"\"\n",
        "    all_sequences = []\n",
        "    # Loop over the training folders (Train000, Train001, ...)\n",
        "    for f in sorted(listdir(Config.DATASET_PATH)):\n",
        "        directory_path = join(Config.DATASET_PATH, f)\n",
        "        if isdir(directory_path):\n",
        "            all_frames = []\n",
        "            # Loop over all the images in the folder (0.tif, 1.tif, ..., 199.tif)\n",
        "            for c in sorted(listdir(directory_path)):\n",
        "                img_path = join(directory_path, c)\n",
        "                if str(img_path)[-3:] == \"tif\":\n",
        "                    img = Image.open(img_path).resize((256, 256))\n",
        "                    img = np.array(img, dtype=np.float32) / 256.0\n",
        "                    all_frames.append(img)\n",
        "            # Get the 10-frame sequences from the list of images after applying data augmentation\n",
        "            for stride in range(1, 3):\n",
        "                all_sequences.extend(get_clips_by_stride(stride=stride, frames_list=all_frames, sequence_size=10))\n",
        "    return all_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKrcWqBswdNm",
        "outputId": "3f17ba34-6de4-439a-cba0-22201408ad21"
      },
      "outputs": [],
      "source": [
        "pip install keras_layer_normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9Ip6bGjOnaH"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Conv2DTranspose, ConvLSTM2D, BatchNormalization, TimeDistributed, Conv2D\n",
        "from keras.models import Sequential, load_model\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "\n",
        "def get_model(reload_model=True):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      \"\"\"Load or retrain the model.\"\"\"\n",
        "      if not reload_model:\n",
        "          return load_model(Config.MODEL_PATH, custom_objects={'LayerNormalization': LayerNormalization})\n",
        "      training_set = get_dataset()\n",
        "      training_set = np.array(training_set)\n",
        "      seq = Sequential()\n",
        "      seq.add(TimeDistributed(Conv2D(128, (4, 4), strides=2, padding=\"same\"), batch_input_shape=(None, 10, 256, 256, 1)))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(TimeDistributed(Conv2D(64, (2, 2), strides=2, padding=\"same\")))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(TimeDistributed(Conv2D(32, (3, 3), strides=2, padding=\"same\")))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(ConvLSTM2D(32, (3, 3), padding=\"same\", return_sequences=True))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(TimeDistributed(Conv2DTranspose(32, (3, 3), strides=2, padding=\"same\")))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(TimeDistributed(Conv2DTranspose(64, (2, 2), strides=2, padding=\"same\")))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(TimeDistributed(Conv2DTranspose(128, (4, 4), strides=2, padding=\"same\")))\n",
        "      seq.add(LayerNormalization())\n",
        "      seq.add(TimeDistributed(Conv2D(1, (4, 4), activation=\"sigmoid\", padding=\"same\")))\n",
        "      print(seq.summary())\n",
        "      optimizer = keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-6)\n",
        "      seq.compile(loss='mse', optimizer=optimizer)\n",
        "      print(\"Compiling done\")\n",
        "      seq.fit(training_set, training_set,\n",
        "              batch_size=Config.BATCH_SIZE, epochs=Config.EPOCHS, shuffle=False)\n",
        "      seq.save(Config.MODEL_PATH)\n",
        "      print(\"Training Done\")\n",
        "    return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CewbyNwnOpiD"
      },
      "outputs": [],
      "source": [
        "def get_single_test():\n",
        "    \"\"\"Load single test data.\"\"\"\n",
        "    sz = 200\n",
        "    test = np.zeros(shape=(sz, 256, 256, 1))\n",
        "    cnt = 0\n",
        "    for f in sorted(listdir(Config.SINGLE_TEST_PATH)):\n",
        "        if str(join(Config.SINGLE_TEST_PATH, f))[-3:] == \"tif\":\n",
        "            img = Image.open(join(Config.SINGLE_TEST_PATH, f)).resize((256, 256))\n",
        "            img = np.array(img, dtype=np.float32) / 256.0\n",
        "            test[cnt, :, :, 0] = img\n",
        "            cnt = cnt + 1\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5IpwBco27brp",
        "outputId": "97532750-ca06-4657-bd61-84f98d5c5716"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def evaluate():\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model = get_model(True)\n",
        "    print(\"Got model\")\n",
        "    test = get_single_test()\n",
        "    print(\"Got test\")\n",
        "    sz = test.shape[0] - 10\n",
        "    sequences = np.zeros((sz, 10, 256, 256, 1))\n",
        "    # Apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((10, 256, 256, 1))\n",
        "        for j in range(0, 10):\n",
        "            clip[j] = test[i + j, :, :, :]\n",
        "        sequences[i] = clip\n",
        "\n",
        "    # Get the reconstruction cost of all the sequences\n",
        "    reconstructed_sequences = model.predict(sequences, batch_size=4)\n",
        "    sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(sequences[i],reconstructed_sequences[i])) for i in range(0, sz)])\n",
        "    sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / np.max(sequences_reconstruction_cost)\n",
        "    sr = 1.0 - sa\n",
        "\n",
        "    # Plot the regularity scores\n",
        "    plt.plot(sr)\n",
        "    plt.ylabel('Regularity score Sr(t)')\n",
        "    plt.xlabel('Frame t')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Get the dataset\n",
        "all_sequences = np.array(get_dataset())\n",
        "\n",
        "# Split the dataset into training and testing data\n",
        "train_data, test_data = train_test_split(all_sequences, test_size=Config.TEST_SIZE, shuffle=False)\n",
        "\n",
        "evaluate()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
